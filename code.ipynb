{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "# Opens and filters a local XML sitemap file and extracts all details\n",
    "def parse_sitemap(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as xml_file:\n",
    "        # Use 'xml' parser for sitemaps\n",
    "        return BeautifulSoup(xml_file, \"xml\").find_all(\"loc\")\n",
    "\n",
    "def get_recipe_urls(locations):\n",
    "    # Filters for URLs containing the specific path \"/recipe/\"\n",
    "    return [i.text for i in locations if \"/recipe/\" in i.text]\n",
    "\n",
    "sitemap = \"C:/Users/Prana/Documents/CGAS-EATLYST/sitemap_1.xml\"\n",
    "\n",
    "# Setting headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "try:\n",
    "    locations = parse_sitemap(sitemap)\n",
    "    all_recipe_urls = get_recipe_urls(locations)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Sitemap file not found at {sitemap}\")\n",
    "    exit()\n",
    "\n",
    "# I have limited the sample size to avoid hitting the server too hard and randomly chosen 26 recipes\n",
    "sample_size = 26\n",
    "if len(all_recipe_urls) >= sample_size:\n",
    "    urlArray = random.sample(all_recipe_urls, sample_size)\n",
    "else:\n",
    "    urlArray = all_recipe_urls \n",
    "    print(f\"Warning: Only {len(all_recipe_urls)} recipe URLs found, fewer than the requested {sample_size}.\")\n",
    "\n",
    "# Making dataset to store scraped data using beautifulsoup and pandas\n",
    "df = pd.DataFrame(columns=['Recipe_Name', 'Recipe_URL', 'Image_URL', 'Detailed_Ingredients', 'Instructions', 'Prep Time'])\n",
    "\n",
    "# output file path\n",
    "output_file = 'recipe_scraping.csv'\n",
    "\n",
    "for currUrl in urlArray:\n",
    "    time.sleep(random.uniform(1, 3)) \n",
    "    \n",
    "    try:\n",
    "        r = requests.get(currUrl, headers=headers)\n",
    "        r.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "\n",
    "    #Extract Recipe Name\n",
    "        recipeName = soup.find('h1')\n",
    "        recipeName = recipeName.get_text(\" \", strip=True) if recipeName else \"N/A\"\n",
    "\n",
    "    # Extract Image URL \n",
    "        image_tag = soup.find('img', attrs={'class': 'mm-img-block__img'})\n",
    "        \n",
    "        # Fallback to the first image found if specific class doesn't work (may not be the main image)\n",
    "        if not image_tag:\n",
    "             image_tag = soup.find('img')\n",
    "\n",
    "        # Extract the URL from the 'src' attribute\n",
    "        image_url = image_tag.get('src') if image_tag and image_tag.get('src') else \"N/A\"\n",
    "\n",
    "\n",
    "    #Extract Ingredients \n",
    "        ingredientDiv = soup.find_all('li', attrs={'class': 'mm-recipes-structured-ingredients__list-item'})\n",
    "        recipeIngredient = []\n",
    "        for row in ingredientDiv:\n",
    "            spans = row.find_all('span')\n",
    "            if spans:\n",
    "                ingredient = ' '.join(s.get_text(\" \", strip=True) for s in spans if s.get_text(strip=True))\n",
    "            else:\n",
    "                ingredient = row.get_text(\" \", strip=True)\n",
    "            \n",
    "            # Clean up extra spaces\n",
    "            ingredient = ' '.join(ingredient.split()) \n",
    "            if ingredient:\n",
    "                recipeIngredient.append(ingredient)\n",
    "                \n",
    "        recipeIngredient = ', '.join(recipeIngredient) if recipeIngredient else \"N/A\"\n",
    "\n",
    "    #Extract Instructions ---\n",
    "        recipeInstructionDiv = soup.find_all('li', attrs={\n",
    "            'class': 'comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI'\n",
    "        })\n",
    "\n",
    "        steps = []\n",
    "        for row in recipeInstructionDiv:\n",
    "            p = row.find('p')\n",
    "            # Get text from <p> or the list item itself\n",
    "            text = (p.get_text(\" \", strip=True) if p else row.get_text(\" \", strip=True)).replace('\\n', '')\n",
    "            text = ' '.join(text.split())\n",
    "            if text:\n",
    "                steps.append(text)\n",
    "                \n",
    "        recipeInstruction = \" \".join(steps) if steps else \"N/A\"\n",
    "\n",
    "        details = soup.find('div', class_='mm-recipes-details__content')\n",
    "        Total_time = \"N/A\"\n",
    "        if details:\n",
    "            # Find the 'Total Time:' label\n",
    "            Total_label = details.find('div', string='Total Time:')\n",
    "            if Total_label:\n",
    "                # Find the next sibling div with the value\n",
    "                Total_time_value = Total_label.find_next('div', class_='mm-recipes-details__value')\n",
    "                Total_time = Total_time_value.get_text(\" \", strip=True) if Total_time_value else \"N/A\"\n",
    "\n",
    "        new_row_df = pd.DataFrame([{\n",
    "            'Recipe_Name': recipeName,\n",
    "            'Recipe_URL': currUrl,\n",
    "            'Image_URL': image_url, \n",
    "            'Detailed_Ingredients': recipeIngredient,\n",
    "            'Instructions': recipeInstruction,\n",
    "            'Prep Time': Total_time,\n",
    "        }])\n",
    "\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Processed: {currUrl} | Image URL: {image_url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If a link fails, move to the next one\n",
    "        print(f\" Error fetching {currUrl}: {e}\")\n",
    "        continue \n",
    "    except Exception as e:\n",
    "        print(f\" An unexpected error occurred while processing {currUrl}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n--- Scraping Complete ---\")\n",
    "print(f\"Data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
